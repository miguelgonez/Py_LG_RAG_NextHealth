"""
DemostraciÃ³n rÃ¡pida de LangGraph RAG con ejemplos prÃ¡cticos
"""

import os
from dotenv import load_dotenv

# Cargar variables de entorno desde .env
load_dotenv()

# Configurar
print("ğŸ”§ Configurando entorno...")
os.environ["PERSIST_DIR"] = "./db/chroma"
os.environ["SQLITE_PATH"] = "./db/icpc3.db"

print("=" * 80)
print("ğŸ§  DEMO: RAG NextHealth - LangGraph Edition")
print("=" * 80)

# Verificar que la API key estÃ¡ configurada
if not os.getenv("OPENAI_API_KEY"):
    print("\nâš ï¸ IMPORTANTE: Configura tu OPENAI_API_KEY!")
    print("Crea un archivo .env con:")
    print("OPENAI_API_KEY=sk-...")
    print("\nO ejecuta: export OPENAI_API_KEY='sk-...'\n")
    exit(1)

print(f"âœ… API Key cargada: {os.getenv('OPENAI_API_KEY')[:10]}...")

from src.langgraph_rag import invoke_rag
import json


def demo_conversation():
    """Demo de conversaciÃ³n con memoria"""
    print("\n" + "=" * 80)
    print("ğŸ“ DEMO 1: ConversaciÃ³n Multi-turno con Memoria")
    print("=" * 80)

    conversation_id = "demo-001"

    # Pregunta 1
    print("\nğŸ‘¤ Usuario: Â¿QuÃ© es la diabetes mellitus tipo 2?")
    result1 = invoke_rag(
        question="Â¿QuÃ© es la diabetes mellitus tipo 2?",
        conversation_id=conversation_id,
        chat_history=[]
    )

    if result1:
        print(f"\nğŸ¤– Asistente:")
        print(f"Ruta: {result1['route']}")
        print(f"Relevancia: {result1['relevance_score']:.2f}")
        print(f"Respuesta (primeros 300 chars):")
        print(result1['response'][:300] + "...")

        # Pregunta 2 (seguimiento)
        print("\n" + "-" * 80)
        print("\nğŸ‘¤ Usuario: Â¿CuÃ¡les son sus sÃ­ntomas principales?")
        result2 = invoke_rag(
            question="Â¿CuÃ¡les son sus sÃ­ntomas principales?",
            conversation_id=conversation_id,
            chat_history=result1['chat_history']  # â† Memoria!
        )

        if result2:
            print(f"\nğŸ¤– Asistente:")
            print(f"Query contextualizada: {result2['question']}")
            print(f"Respuesta (primeros 300 chars):")
            print(result2['response'][:300] + "...")

            # Pregunta 3
            print("\n" + "-" * 80)
            print("\nğŸ‘¤ Usuario: Â¿Y en niÃ±os?")
            result3 = invoke_rag(
                question="Â¿Y en niÃ±os?",
                conversation_id=conversation_id,
                chat_history=result2['chat_history']  # â† Memoria acumulada!
            )

            if result3:
                print(f"\nğŸ¤– Asistente:")
                print(f"Query contextualizada: {result3['question']}")
                print(f"Historial: {len(result3['chat_history'])} intercambios")

    print("\nâœ… Demo de conversaciÃ³n completada")


def demo_crag():
    """Demo de CRAG con refinamiento automÃ¡tico"""
    print("\n" + "=" * 80)
    print("ğŸ”„ DEMO 2: CRAG - Refinamiento AutomÃ¡tico")
    print("=" * 80)

    # Query ambigua
    print("\nğŸ‘¤ Usuario: tratamiento")
    print("(Query muy vaga, deberÃ­a trigger refinamiento automÃ¡tico)")

    result = invoke_rag(
        question="tratamiento",
        conversation_id="demo-002",
        retrieval_mode="crag"
    )

    if result:
        print(f"\nğŸ¤– Sistema:")
        print(f"Iteraciones de refinamiento: {result['retrieval_iteration']}")
        print(f"Relevancia final: {result['relevance_score']:.2f}")
        print(f"NecesitÃ³ refinamiento: {result.get('needs_refinement', False)}")
        print(f"Flujo de nodos: {' â†’ '.join(result['node_sequence'])}")

        if result['retrieval_iteration'] > 0:
            print(f"\nâœ… CRAG refinÃ³ automÃ¡ticamente la consulta")

    print("\nâœ… Demo de CRAG completada")


def demo_routing():
    """Demo de routing SQL vs Vector"""
    print("\n" + "=" * 80)
    print("ğŸ§­ DEMO 3: Routing Inteligente (SQL vs Vector)")
    print("=" * 80)

    queries = [
        ("Â¿CuÃ¡l es el cÃ³digo ICPC-3 para hipertensiÃ³n?", "sql"),
        ("Â¿QuÃ© es la hipertensiÃ³n arterial?", "vector"),
        ("Lista todos los cÃ³digos de diabetes", "sql"),
        ("Â¿CÃ³mo se trata la diabetes tipo 2?", "vector")
    ]

    for query, expected_route in queries:
        print(f"\nğŸ‘¤ Query: {query}")
        print(f"   Ruta esperada: {expected_route}")

        result = invoke_rag(
            question=query,
            conversation_id="demo-003"
        )

        if result:
            actual_route = result['route']
            confidence = result['route_confidence']

            status = "âœ…" if actual_route == expected_route else "âš ï¸"
            print(f"   {status} Ruta detectada: {actual_route} (confianza: {confidence:.2f})")

    print("\nâœ… Demo de routing completada")


def demo_latency():
    """Demo de mÃ©tricas de latencia"""
    print("\n" + "=" * 80)
    print("â±ï¸ DEMO 4: MÃ©tricas de Latencia por Nodo")
    print("=" * 80)

    print("\nğŸ‘¤ Usuario: Â¿QuÃ© es la diabetes?")

    result = invoke_rag(
        question="Â¿QuÃ© es la diabetes?",
        conversation_id="demo-004"
    )

    if result:
        print(f"\nğŸ“Š Latencias:")

        latencies = result.get('latency_ms', {})
        total = sum(latencies.values())

        # Ordenar por tiempo
        sorted_latencies = sorted(latencies.items(), key=lambda x: x[1], reverse=True)

        for node, ms in sorted_latencies:
            percentage = (ms / total * 100) if total > 0 else 0
            bar_length = int(percentage / 2)  # Scale para visualizaciÃ³n
            bar = "â–ˆ" * bar_length

            print(f"  {node:15s} {ms:7.1f}ms  {bar} {percentage:.1f}%")

        print(f"\n  {'TOTAL':15s} {total:7.1f}ms")

    print("\nâœ… Demo de latencia completada")


def demo_self_rag():
    """Demo de Self-RAG evaluation"""
    print("\n" + "=" * 80)
    print("ğŸ¯ DEMO 5: Self-RAG - Auto-evaluaciÃ³n")
    print("=" * 80)

    queries = [
        "Â¿CuÃ¡les son los sÃ­ntomas de la diabetes tipo 2?",  # Buena pregunta
        "xyz123abc"  # Pregunta sin sentido
    ]

    for query in queries:
        print(f"\nğŸ‘¤ Query: {query}")

        result = invoke_rag(
            question=query,
            conversation_id="demo-005"
        )

        if result:
            print(f"   Relevancia: {result['relevance_score']:.2f}")
            print(f"   Puede responder: {result['can_answer']}")
            print(f"   Necesita refinamiento: {result.get('needs_refinement', False)}")
            print(f"   Documentos encontrados: {len(result.get('reranked_docs', []))}")

    print("\nâœ… Demo de Self-RAG completada")


def main():
    """Ejecuta todas las demos"""
    print("\n")
    print("â•”" + "â•" * 78 + "â•—")
    print("â•‘" + " " * 20 + "ğŸ§  RAG NextHealth - LangGraph" + " " * 28 + "â•‘")
    print("â•‘" + " " * 25 + "DemostraciÃ³n Interactiva" + " " * 30 + "â•‘")
    print("â•š" + "â•" * 78 + "â•")

    try:
        # Demo 1: ConversaciÃ³n
        demo_conversation()

        # Demo 2: CRAG
        demo_crag()

        # Demo 3: Routing
        demo_routing()

        # Demo 4: Latencia
        demo_latency()

        # Demo 5: Self-RAG
        demo_self_rag()

        # Resumen final
        print("\n" + "=" * 80)
        print("ğŸ‰ Â¡TODAS LAS DEMOS COMPLETADAS!")
        print("=" * 80)

        print("""
âœ¨ Features demostradas:
  âœ… Memoria conversacional multi-turno
  âœ… CRAG con refinamiento automÃ¡tico
  âœ… Routing inteligente SQL/Vector
  âœ… MÃ©tricas de latencia por nodo
  âœ… Self-RAG auto-evaluaciÃ³n

ğŸ“š PrÃ³ximos pasos:
  1. Ejecuta: streamlit run app_langgraph.py
  2. Prueba tus propias queries
  3. Explora el flujo de nodos en la interfaz
  4. Revisa LANGGRAPH_GUIDE.md para mÃ¡s detalles

ğŸ”— DocumentaciÃ³n:
  - LANGGRAPH_GUIDE.md
  - README.md
        """)

    except Exception as e:
        print(f"\nâŒ Error durante las demos: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
