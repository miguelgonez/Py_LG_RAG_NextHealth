# ðŸ§  RAG NextHealth - LangGraph Edition

## GuÃ­a Completa de LangGraph Integration

### ðŸš€ Â¿QuÃ© es nuevo?

Esta versiÃ³n mejorada de RAG NextHealth integra **LangGraph** para orquestaciÃ³n avanzada del flujo RAG, aÃ±adiendo:

1. **Memoria Conversacional**: Mantiene contexto entre preguntas
2. **Self-RAG**: Auto-evaluaciÃ³n de la calidad de respuestas
3. **CRAG Mejorado**: Ciclos de correcciÃ³n automÃ¡tica (hasta 2 iteraciones)
4. **Checkpointing**: RecuperaciÃ³n automÃ¡tica ante fallos
5. **Flujo Visual**: Trazabilidad completa del proceso

---

## ðŸ“Š Arquitectura del Grafo

```
User Query
    â†“
[Contextualize] â†’ Reformula con historial conversacional
    â†“
[Route] â†’ Decide: SQL o Vector?
    â†“
[Retrieve] â†’ Recupera documentos (Fusion/RAPTOR/HYDE/CRAG)
    â†“
[Rerank] â†’ BGE cross-encoder (top 4)
    â†“
[Evaluate] â†’ Â¿Relevancia suficiente?
    â†“ (NO, relevancia < 0.4)
[Refine] â†’ Reformula query mÃ©dica
    â†“
[Retrieve] â†’ Segundo intento â† CICLO!
    â†“ (SÃ, relevancia â‰¥ 0.6)
[Generate] â†’ Respuesta final + Guardrails
    â†“
[Update Memory] â†’ Actualiza historial
```

---

## ðŸ”§ InstalaciÃ³n

### Dependencias adicionales

```bash
pip install langgraph langgraph-checkpoint-sqlite
```

Todas las demÃ¡s dependencias ya estÃ¡n en `requirements.txt`.

---

## ðŸŽ¯ Uso

### OpciÃ³n 1: Streamlit App (Recomendado)

```bash
streamlit run app_langgraph.py --server.port 5000
```

**Features de la interfaz:**
- Chat conversacional con memoria
- VisualizaciÃ³n del flujo de nodos
- MÃ©tricas de latencia por nodo
- Nueva conversaciÃ³n con un click
- Tracking de iteraciones CRAG

### OpciÃ³n 2: Python Script

```python
from src.langgraph_rag import invoke_rag

# Primera pregunta
result = invoke_rag(
    question="Â¿CuÃ¡les son los sÃ­ntomas de la diabetes tipo 2?",
    conversation_id="user-123",
    chat_history=[],
    retrieval_mode="crag"
)

print(result["response"])

# Pregunta de seguimiento
result2 = invoke_rag(
    question="Â¿Y en niÃ±os?",  # Contexto automÃ¡tico!
    conversation_id="user-123",
    chat_history=result["chat_history"],  # Mantiene memoria
    retrieval_mode="crag"
)
```

### OpciÃ³n 3: Pruebas

```bash
python test_langgraph.py
```

Ejecuta suite de tests:
- âœ… InvocaciÃ³n bÃ¡sica
- âœ… Memoria conversacional
- âœ… CRAG refinamiento
- âœ… SQL routing
- âœ… Estructura del grafo

---

## ðŸ§© Componentes del Grafo

### Nodos

#### 1. **Contextualize**
```python
def contextualize_question(state: RAGState) -> RAGState
```
- Reformula pregunta usando historial conversacional
- Crea queries "standalone" con contexto implÃ­cito
- **Ejemplo:**
  - User 1: "Â¿QuÃ© es diabetes?"
  - User 2: "Â¿Y en niÃ±os?" â†’ "Â¿QuÃ© es la diabetes tipo 1 en niÃ±os?"

#### 2. **Route**
```python
def route_query(state: RAGState) -> RAGState
```
- Decide automÃ¡ticamente: SQL vs Vector
- Detecta cÃ³digos ICPC-3, CIE-10, consultas estructuradas
- **Salida:** `route: "sql" | "vector"`

#### 3. **Retrieve**
```python
def retrieve_documents(state: RAGState) -> RAGState
```
- Ejecuta estrategia configurada:
  - `standard`: Multi-query + Fusion
  - `raptor`: JerÃ¡rquico con sumarios
  - `rag-fusion`: RRF combinado
  - `hyde`: Documentos hipotÃ©ticos
  - `crag`: CorrecciÃ³n automÃ¡tica
- **K inicial:** 8 documentos

#### 4. **Rerank**
```python
def rerank_documents(state: RAGState) -> RAGState
```
- BGE cross-encoder re-ranking
- **K final:** 4 documentos mÃ¡s relevantes
- Scores de relevancia en metadata

#### 5. **Evaluate** (Self-RAG)
```python
def evaluate_relevance(state: RAGState) -> RAGState
```
- Auto-evaluaciÃ³n de relevancia (0-1)
- **Umbral:** 0.6 para "can_answer"
- **Umbral refinamiento:** < 0.4 para "needs_refinement"

#### 6. **Refine** (condicional)
```python
def refine_query(state: RAGState) -> RAGState
```
- Reformula query con terminologÃ­a mÃ©dica precisa
- Solo si relevancia < 0.4
- **LÃ­mite:** 2 iteraciones mÃ¡ximo

#### 7. **Generate**
```python
def generate_response(state: RAGState) -> RAGState
```
- Genera respuesta con LLM
- Aplica guardrails mÃ©dicos (MDR/AI Act)
- Actualiza memoria conversacional
- AÃ±ade disclaimer automÃ¡tico

### Edges Condicionales

```python
def should_refine_query(state: RAGState) -> Literal["refine", "generate"]:
    """
    Decide si refinar o generar basado en:
    - Relevancia < 0.4 â†’ Refinar
    - Iteraciones >= 2 â†’ Generar (lÃ­mite)
    - Relevancia >= 0.6 â†’ Generar
    """
```

**Esto crea CICLOS** en el grafo: `evaluate â†’ refine â†’ retrieve â†’ rerank â†’ evaluate`

---

## ðŸ“ˆ Estado del Grafo

```python
class RAGState(TypedDict):
    # Input
    question: str                     # Query actual (puede ser reformulada)
    original_question: str            # Query original del usuario
    conversation_id: str              # ID de sesiÃ³n

    # Memoria
    messages: Sequence[BaseMessage]   # Historial formato LangChain
    chat_history: list[tuple]         # [(user, assistant), ...]

    # Routing
    route: str                        # "sql" | "vector"
    route_confidence: float           # 0-1
    route_reasoning: str              # ExplicaciÃ³n

    # Retrieval
    documents: list[Document]         # Docs recuperados (k=8)
    retrieval_method: str             # "fusion" | "raptor" | etc
    retrieval_iteration: int          # 0, 1, 2 (CRAG)

    # Reranking
    reranked_docs: list[Document]     # Top 4 docs

    # Evaluation
    relevance_score: float            # 0-1
    can_answer: bool                  # â‰¥ 0.6?
    needs_refinement: bool            # < 0.4?

    # Generation
    response: str                     # Respuesta final

    # Metadata
    error: str | None                 # Error si ocurriÃ³
    latency_ms: dict                  # Latencias por nodo
    node_sequence: list[str]          # Secuencia ejecutada
```

---

## ðŸ” Ejemplos de Uso

### Ejemplo 1: ConversaciÃ³n Multi-turno

```python
from src.langgraph_rag import invoke_rag

# Turno 1
r1 = invoke_rag(
    question="Â¿QuÃ© es la hipertensiÃ³n arterial?",
    conversation_id="patient-001",
    chat_history=[]
)

# Turno 2 (con contexto)
r2 = invoke_rag(
    question="Â¿CuÃ¡l es el tratamiento?",
    # Sistema entiende: "Â¿CuÃ¡l es el tratamiento de la hipertensiÃ³n arterial?"
    conversation_id="patient-001",
    chat_history=r1["chat_history"]
)

# Turno 3
r3 = invoke_rag(
    question="Â¿Y efectos secundarios?",
    # Sistema entiende contexto de tratamiento de hipertensiÃ³n
    conversation_id="patient-001",
    chat_history=r2["chat_history"]
)
```

### Ejemplo 2: CRAG AutomÃ¡tico

```python
# Query ambigua â†’ Trigger refinamiento
result = invoke_rag(
    question="sÃ­ntomas",  # Muy vaga
    retrieval_mode="crag"
)

# Sistema:
# 1. Retrieve con "sÃ­ntomas" â†’ Relevancia 0.2 (baja)
# 2. Refine â†’ "Â¿CuÃ¡les son los sÃ­ntomas comunes de enfermedades crÃ³nicas?"
# 3. Retrieve nuevamente â†’ Relevancia 0.7 (buena)
# 4. Generate respuesta

print(result["retrieval_iteration"])  # 1 (refinÃ³ una vez)
print(result["relevance_score"])      # 0.7
```

### Ejemplo 3: Routing AutomÃ¡tico

```python
# Query SQL
result_sql = invoke_rag(
    question="Â¿CÃ³digo ICPC-3 para diabetes tipo 2?"
)
print(result_sql["route"])  # "sql"

# Query Vector
result_vec = invoke_rag(
    question="Â¿CÃ³mo se diagnostica la diabetes tipo 2?"
)
print(result_vec["route"])  # "vector"
```

---

## âš™ï¸ ConfiguraciÃ³n

### Variables de Entorno

```bash
# Requerido
export OPENAI_API_KEY="sk-..."

# Opcional
export PERSIST_DIR="./db/chroma"
export SQLITE_PATH="./db/icpc3.db"
export RETRIEVAL_MODE="crag"  # standard|raptor|rag-fusion|hyde|crag
export OPENAI_CHAT_MODEL="gpt-5"
export EMBEDDINGS_MODEL="intfloat/multilingual-e5-base"
```

### Checkpointing

El sistema guarda checkpoints automÃ¡ticamente en:
```
./db/langgraph_checkpoints.db
```

Esto permite:
- âœ… RecuperaciÃ³n ante fallos
- âœ… InspecciÃ³n de estados intermedios
- âœ… Replay de conversaciones

---

## ðŸ“Š MÃ©tricas y Debugging

### Ver Secuencia de Nodos

```python
result = invoke_rag(question="...")
print(result["node_sequence"])
# ['contextualize', 'route', 'retrieve', 'rerank', 'evaluate', 'generate']
```

### Latencias por Nodo

```python
for node, latency in result["latency_ms"].items():
    print(f"{node}: {latency:.1f}ms")

# Ejemplo:
# contextualize: 234.5ms
# route: 12.3ms
# retrieve: 1234.5ms
# rerank: 456.7ms
# evaluate: 789.0ms
# generate: 2345.6ms
```

### Debugging en LangSmith

LangGraph se integra automÃ¡ticamente con LangSmith:

```bash
export LANGCHAIN_TRACING_V2=true
export LANGCHAIN_API_KEY="your-key"
export LANGCHAIN_PROJECT="rag-nexthealth"
```

Visualiza el grafo completo en https://smith.langchain.com

---

## ðŸ†š ComparaciÃ³n: Original vs LangGraph

| Feature | Original (graph.py) | LangGraph (langgraph_rag.py) |
|---------|-------------------|----------------------------|
| Memoria conversacional | âŒ | âœ… |
| Self-RAG | âŒ | âœ… |
| CRAG con ciclos | âš ï¸ BÃ¡sico | âœ… Avanzado (hasta 2 intentos) |
| Checkpointing | âŒ | âœ… |
| Estado persistente | âŒ | âœ… |
| Flujo visual | âŒ | âœ… |
| Edges condicionales | âš ï¸ Simple | âœ… Completo |
| Debugging | âš ï¸ Limitado | âœ… LangSmith integration |

---

## ðŸŽ“ TÃ©cnicas Avanzadas Implementadas

### 1. **Contextual Compression**
```python
# En contextualize_question()
# Combina historial + query actual â†’ query standalone
```

### 2. **Adaptive Retrieval**
```python
# En evaluate_relevance()
# Decide automÃ¡ticamente si necesita mÃ¡s documentos
```

### 3. **Query Refinement Loop**
```python
# evaluate â†’ refine â†’ retrieve â†’ evaluate (mÃ¡x 2 ciclos)
```

### 4. **Multi-stage Reranking**
```python
# retrieve (k=8) â†’ BGE rerank (k=4)
```

### 5. **Guardrails Integration**
```python
# PolÃ­ticas mÃ©dicas MDR/AI Act aplicadas automÃ¡ticamente
```

---

## ðŸ› Troubleshooting

### Error: "No se pudo cargar el retriever"

**SoluciÃ³n:**
```bash
# AsegÃºrate de que el vector store existe
ls ./db/chroma

# Si no existe, ejecuta ingesta:
python -c "from src.ingest import main; main('./db/chroma', './docs')"
```

### Error: "OpenAI API key not configured"

**SoluciÃ³n:**
```bash
export OPENAI_API_KEY="sk-your-key-here"
```

### Relevancia siempre baja

**Posibles causas:**
1. Vector store vacÃ­o â†’ Ejecuta ingesta
2. Query muy vaga â†’ Sistema refinarÃ¡ automÃ¡ticamente
3. Modelo de embeddings diferente â†’ Verifica configuraciÃ³n

---

## ðŸ“š Recursos Adicionales

- **LangGraph Docs:** https://langchain-ai.github.io/langgraph/
- **LangSmith:** https://smith.langchain.com
- **Paper CRAG:** https://arxiv.org/abs/2401.15884
- **Paper Self-RAG:** https://arxiv.org/abs/2310.11511

---

## ðŸ¤ Contribuciones

El sistema LangGraph estÃ¡ diseÃ±ado para ser extensible. Puedes aÃ±adir:

- Nuevos nodos (ej: `web_search_fallback`)
- Nuevas condiciones en edges
- Nuevas estrategias de recuperaciÃ³n
- Nuevos evaluadores de relevancia

---

## ðŸ“„ Licencia

MIT License - Ver archivo LICENSE

---

## ðŸŽ‰ PrÃ³ximos Pasos

1. âœ… LangGraph implementado
2. âœ… Memoria conversacional
3. âœ… Self-RAG
4. âœ… CRAG con ciclos
5. â³ Web search fallback
6. â³ Multi-agent orchestration
7. â³ Streaming responses
8. â³ Voice interface

---

**VersiÃ³n:** 2.0.0 (LangGraph Edition)
**Autor:** RAG NextHealth Team
**Fecha:** 2025
